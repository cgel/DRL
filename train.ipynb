{
 "metadata": {
  "name": "",
  "signature": "sha256:943017037cd009f27cca403294fd08408bd13602b67eb0b214660a7eb99a0583"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from ale_python_interface import ALEInterface\n",
      "import tensorflow as tf\n",
      "import numpy as np\n",
      "import cv2\n",
      "import random\n",
      "import threading\n",
      "\n",
      "import sys\n",
      "import time\n",
      "import os\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "\n",
      "from replayMemory import ReplayMemory\n",
      "from buildGraph import createQNetwork, build_train_op"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/home/cgel/.local/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
        "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ale = ALEInterface()\n",
      "viz = False\n",
      "rom_name = \"roms/Breakout.bin\"\n",
      "ale.setBool('sound', False)                                                                                \n",
      "ale.setBool('display_screen', viz) \n",
      "ale.setInt(\"frame_skip\", 4)\n",
      "ale.loadROM(rom_name)\n",
      "legal_actions = ale.getMinimalActionSet()\n",
      "action_map = {}\n",
      "for i in range(len(legal_actions)):\n",
      "    action_map[i] = legal_actions[i]\n",
      "action_num = len(action_map)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "STEPS_BEFORE_TRAINING = 50000\n",
      "EXPLORE = 1000000. # frames over which to anneal epsilon\n",
      "FINAL_EPSILON = 0.1#0.001 # final value of epsilon\n",
      "INITIAL_EPSILON = 1.0#0.01 # starting value of epsilon\n",
      "BATCH_SIZE = 32 # size of minibatch\n",
      "PARAM_SYNC_STEPS = 10000\n",
      "BUF_SIZE = 4\n",
      "SAVE_SUMMARY_STEPS = 10000\n",
      "\n",
      "class config:\n",
      "    batch_size = 32\n",
      "    replay_memory_capacity = 1000000\n",
      "    buff_size = BUF_SIZE\n",
      "    \n",
      "def get_epsilon():\n",
      "    if global_step < EXPLORE:\n",
      "        return INITIAL_EPSILON - ( (INITIAL_EPSILON-FINAL_EPSILON)/EXPLORE ) * global_step\n",
      "    else:\n",
      "        return FINAL_EPSILON\n",
      "    \n",
      "RM = ReplayMemory(config)\n",
      "\n",
      "learning_rate = 0.00025\n",
      "GAMMA = 0.96\n",
      "\n",
      "def flush_print(str):\n",
      "    print(str)\n",
      "    sys.stdout.flush()\n",
      "    \n",
      "def preprocess(new_frame, state):\n",
      "    frame = cv2.resize(new_frame, (84, 110))[26:110,:]\n",
      "    new_state = np.roll(state, -1, axis=3)\n",
      "    new_state[0, :, :, BUF_SIZE -1] = frame\n",
      "    return new_state"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with tf.device(\"/gpu:0\"):\n",
      "    input_state_ph = tf.placeholder(tf.float32,[config.batch_size,84,84,4], name=\"input_state_placeholder\")\n",
      "    # this should be: input_state_placeholder = tf.placeholder(\"float\",[None,84,84,4], name=\"state_placeholder\")\n",
      "    action_ph = tf.placeholder(tf.int64, [config.batch_size], name=\"Action_placholder\")\n",
      "    Y_ph = tf.placeholder(tf.float32, [config.batch_size], name=\"Y_placeholder\")\n",
      "    next_Y_ph = tf.placeholder(tf.float32, [config.batch_size, action_num], name=\"next_Y_placeholder\")\n",
      "    ph_lst = [input_state_ph, action_ph, Y_ph, next_Y_ph]\n",
      "    q = tf.FIFOQueue(10, [ph.dtype for ph in ph_lst], [ph.get_shape() for ph in ph_lst])\n",
      "    enqueue_op = q.enqueue([input_state_ph, action_ph, Y_ph, next_Y_ph])\n",
      "    input_state, action, Y, next_Y = q.dequeue()\n",
      "    # so that i can feed inputs with different batch sizes. \n",
      "    input_state = tf.placeholder_with_default(input_state, shape=tf.TensorShape([None]).concatenate(input_state.get_shape()[1:]))\n",
      "    next_input_state_ph = tf.placeholder(tf.float32,[config.batch_size,84,84,4], name=\"next_input_state_placeholder\")\n",
      "    \n",
      "    with tf.variable_scope(\"DQN\"):\n",
      "        Q, predicted_next_Q, DQN_params = createQNetwork(input_state, action, action_num, \"DQN_summaries\")\n",
      "        max_action_DQN = tf.argmax(Q, 1)\n",
      "    with tf.variable_scope(\"DQNTarget\"):\n",
      "        # pasing an action is useless because the target never runs the next_Y_prediction but it is needed for the code to work\n",
      "        QT, predicted_next_QT, DQNT_params = createQNetwork(next_input_state_ph, action, action_num, \"DQNT_summaries\")\n",
      "\n",
      "    # DQN summary\n",
      "    for i in range(action_num):\n",
      "        dqni = tf.scalar_summary(\"DQN/action\"+str(i), Q[0, i])\n",
      "        tf.add_to_collection(\"DQN_summaries\", dqni)\n",
      "\n",
      "    sync_DQNT_op = [DQNT_params[i].assign(DQN_params[i]) for i in range(len(DQN_params))]\n",
      "\n",
      "    train_op = build_train_op(Q, Y, action, predicted_next_Q, next_Y, action_num, learning_rate)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def enqueue_from_RM():\n",
      "    while True:\n",
      "        state_batch, action_batch, reward_batch, next_state_batch, terminal_batch, _ = RM.sample_transition_batch()\n",
      "\n",
      "        if global_step % SAVE_SUMMARY_STEPS == 0:\n",
      "            QT_np, DQNT_summary_str = sess.run([QT, DQNT_summary_op], feed_dict={next_input_state_ph:next_state_batch})\n",
      "        else:\n",
      "            QT_np = sess.run(QT, feed_dict={next_input_state_ph:next_state_batch})\n",
      "        DQNT_max_action_batch = np.max(QT_np, 1)\n",
      "        Y = []\n",
      "        for i in range(state_batch.shape[0]):\n",
      "            terminal = terminal_batch[i]\n",
      "            if terminal:\n",
      "                Y.append(reward_batch[i])\n",
      "            else:\n",
      "                Y.append(reward_batch[i] + GAMMA * DQNT_max_action_batch[i])\n",
      "\n",
      "        feed_dict={input_state_ph:state_batch, action_ph:action_batch, next_input_state_ph:next_state_batch, Y_ph:Y, next_Y_ph:QT_np}\n",
      "        sess.run(enqueue_op, feed_dict=feed_dict)\n",
      "enqueue_from_RM_thread = threading.Thread(target=enqueue_from_RM)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def update_params():\n",
      "    if global_step == STEPS_BEFORE_TRAINING:\n",
      "        flush_print(\"starting enqueue thread\")\n",
      "        enqueue_from_RM_thread.start()\n",
      "        \n",
      "    if global_step > STEPS_BEFORE_TRAINING:     \n",
      "        if global_step % SAVE_SUMMARY_STEPS == 0:\n",
      "            _, DQN_summary_str = sess.run([train_op, DQN_summary_op])\n",
      "        else:\n",
      "             _ = sess.run(train_op)           \n",
      "            \n",
      "        if global_step % SAVE_SUMMARY_STEPS == 0:\n",
      "            summary_writter.add_summary(DQN_summary_str, global_step)\n",
      "\n",
      "        if global_step % PARAM_SYNC_STEPS == 0:\n",
      "            sess.run(sync_DQNT_op)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "config = tf.ConfigProto()\n",
      "config.allow_soft_placement = True\n",
      "config.gpu_options.allow_growth = True\n",
      "config.log_device_placement = True\n",
      "sess = tf.Session(config=config)\n",
      "saver = tf.train.Saver(max_to_keep = 20)\n",
      "sess.run(tf.initialize_all_variables())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#geneate a new set of paths\n",
      "run_list = os.listdir(\"log\")\n",
      "int_run_list = [int(r) for r in run_list] + [0]\n",
      "run_name = str(max(int_run_list) + 1)\n",
      "#run_name = str(3)\n",
      "checkpoint_path = \"checkpoint/\" + run_name + \".ckpt\"\n",
      "log_path = \"log/\"+ run_name\n",
      "print(run_name)\n",
      "DQN_summary_op = tf.merge_summary(tf.get_collection(\"DQN_summaries\") + tf.get_collection(\"DQN_summaries_prediction\"))\n",
      "DQNT_summary_op = tf.merge_summary(tf.get_collection(\"DQNT_summaries\"))\n",
      "summary_writter = tf.train.SummaryWriter(log_path, sess.graph, flush_secs=20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "82\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def e_greedy_action(epsilon, state):\n",
      "        if np.random.uniform() < epsilon:\n",
      "            action = random.randint(0, action_num - 1)\n",
      "        else:\n",
      "            action = np.argmax(sess.run(Q, feed_dict={input_state:state})[0])\n",
      "        return action\n",
      "    \n",
      "def greedy_run(epsilon, n):\n",
      "    ale.reset_game()\n",
      "    R_list = []\n",
      "    for episode in range(n):\n",
      "        state = np.zeros((1, 84, 84, BUF_SIZE), dtype=np.uint8)\n",
      "        state = preprocess(ale.getScreenGrayscale(), state)\n",
      "        R = 0\n",
      "        while ale.game_over() == False:\n",
      "            action = e_greedy_action(epsilon, state)\n",
      "            reward = ale.act(action_map[action])\n",
      "            state = preprocess(ale.getScreenGrayscale(), state)\n",
      "            R += reward\n",
      "        R_list.append(R)\n",
      "        ale.reset_game()\n",
      "    return R_list"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "global_step = 0\n",
      "global_episode = 0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t = time.time()\n",
      "num_episodes = 10000\n",
      "initial_episode = global_episode\n",
      "for episode in range(global_episode, num_episodes + global_episode):\n",
      "    global state\n",
      "    state = np.zeros((1, 84, 84, BUF_SIZE), dtype=np.uint8)\n",
      "    state = preprocess(ale.getScreenGrayscale(), state)\n",
      "    R = 0\n",
      "    ep_begin_t = time.time()\n",
      "    isTerminal = False\n",
      "    episode_begining_step = global_step\n",
      "    while isTerminal == False:\n",
      "        action = e_greedy_action(get_epsilon(), state)\n",
      "        reward = ale.act(action_map[action])\n",
      "        clipped_reward = max(-1, min(1, reward))\n",
      "        R += reward\n",
      "        if ale.game_over():\n",
      "            isTerminal = True\n",
      "        RM.add(state[0, :, :, BUF_SIZE -1], action, clipped_reward, isTerminal)\n",
      "        update_params()\n",
      "        state = preprocess(ale.getScreenGrayscale(), state)\n",
      "        global_step += 1\n",
      "    ep_duration = time.time() - ep_begin_t\n",
      "    if episode%10 == 0:\n",
      "        episode_online_summary = tf.Summary(value=[tf.Summary.Value(tag=\"online/epsilon\", simple_value=get_epsilon()), \n",
      "                                    tf.Summary.Value(tag=\"online/R\", simple_value=R),\n",
      "                                    tf.Summary.Value(tag=\"online/steps_in_episode\", simple_value= global_step - episode_begining_step),     \n",
      "                                    tf.Summary.Value(tag=\"online/global_step\", simple_value = global_step),           \n",
      "                                    tf.Summary.Value(tag=\"online/ep_duration_seconds\", simple_value=ep_duration)])\n",
      "        summary_writter.add_summary(episode_online_summary, global_episode)\n",
      "    # log percent\n",
      "    if episode%500 == 0:\n",
      "        percent = int(float(episode - initial_episode)/num_episodes * 100)\n",
      "        print(\"%i%% -- epsilon:%.2f\"%(percent, get_epsilon()))\n",
      "        \n",
      "    # save\n",
      "    if episode%1000 == 0 and episode != 0 or num_episodes == episode:\n",
      "        print(\"saving checkpoint at episode \" + str(episode))\n",
      "        saver.save(sess, checkpoint_path, episode)\n",
      "        \n",
      "    # performance summary\n",
      "    if episode%1000 == 0:\n",
      "        R_list = greedy_run(epsilon = 0.05, n=10)\n",
      "        performance_summary = tf.Summary(value=[tf.Summary.Value(tag=\"R/average\", simple_value=sum(R_list)/len(R_list)),\n",
      "                                      tf.Summary.Value(tag=\"R/max\", simple_value=max(R_list)),\n",
      "                                      tf.Summary.Value(tag=\"R/min\", simple_value=min(R_list))])\n",
      "        summary_writter.add_summary(performance_summary, global_step)\n",
      "        \n",
      "    global_episode += 1\n",
      "    ale.reset_game()\n",
      "    \n",
      "print(\"==\")\n",
      "print((time.time() - t)/60)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "i = 2000\n",
      "update_times[i]\n",
      "global_steps[i]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      " saver.save(sess, checkpoint_path, episode)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 24,
       "text": [
        "'checkpoint/91.ckpt-5'"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "RM_path = \"checkpoint/RM_\"+run_name+\"_\"+str(global_step)\n",
      "def RM_save(self):\n",
      "    name_array = zip(['actions', 'rewards',  'screens',  'terminals',  'state_batch',  'next_state_batch'],\n",
      "        [self.actions, self.rewards, self.screens, self.terminals, self.state_batch, self.next_state_batch])\n",
      "    for idx, (name, array) in enumerate(name_array):\n",
      "        np.save(RM_path+name, array)\n",
      "    \n",
      "def RM_load():\n",
      "    self = ReplayMemory(config)\n",
      "    name_array = zip(['actions', 'rewards',  'screens',  'terminals',  'state_batch',  'next_state_batch'],\n",
      "        [self.actions, self.rewards, self.screens, self.terminals, self.state_batch, self.next_state_batch])\n",
      "    self.filled = True\n",
      "    for idx, (name, array) in enumerate(name_array):\n",
      "        array[:] = np.load(RM_path+name+\".npy\")\n",
      "    return self\n",
      "\n",
      "def RM_info(RM):\n",
      "    print(\"current :\"+str(RM.current))\n",
      "    print(\"step :\"+str(RM.step))\n",
      "    print(\"capacity :\"+str(RM.capacity))\n",
      "    print(\"filled :\"+str(RM.filled))\n",
      "    print(\"batch_size :\"+str(RM.batch_size))\n",
      "    \n",
      "print(RM_path)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "checkpoint/RM_29_1640904\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "saver.save(sess, checkpoint_path, episode)\n",
      "print(checkpoint_path)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "checkpoint/62.ckpt\n"
       ]
      }
     ],
     "prompt_number": 238
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "saver.restore(sess, checkpoint_path+\"-\"+str(episode))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 468
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "greedy_run(0.05, 2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 470,
       "text": [
        "[9, 5]"
       ]
      }
     ],
     "prompt_number": 470
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in range(10000):\n",
      "    update_params()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 471
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "color = \"Greys_r\"\n",
      "def show_state(state):\n",
      "    fig = plt.figure()\n",
      "    for i in range(0, 4):\n",
      "        a=fig.add_subplot(1,4,i+1)\n",
      "        plt.axis(\"off\")\n",
      "        plt.title(str(i))\n",
      "        plt.imshow(state[:,:,i], color)\n",
      "\n",
      "def show_frame(frame):\n",
      "    fig = plt.figure()\n",
      "    plt.axis(\"off\")\n",
      "    if len(frame.shape) == 2:\n",
      "        plt.imshow(frame, color)\n",
      "    elif len(frame.shape) == 3:\n",
      "        plt.imshow(frame[:,:,BUF_SIZE -1], color)\n",
      "    elif len(frame.shape) == 4:\n",
      "        plt.imshow(frame[0, :,:,BUF_SIZE -1], color)\n",
      "    else:\n",
      "        print(\"Wrong shape\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 239
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "RM2 = ReplayMemory(config)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 214
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "maxQ = []\n",
      "Qs = []"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ale.reset_game()\n",
      "state = np.zeros((1, 84, 84, BUF_SIZE), dtype=np.uint8)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "c = 0\n",
      "while ale.game_over() == False and c < 10:\n",
      "    raw_frame = ale.getScreenGrayscale()\n",
      "    prev_state = state\n",
      "    state = preprocess(raw_frame, state)\n",
      "    current_DQN =  sess.run(DQN, feed_dict={DQN_input_placeholder:state})[0]\n",
      "    Qs.append(current_DQN)\n",
      "    maxQ.append(np.max(current_DQN))\n",
      "    action = np.argmax(current_DQN)\n",
      "    if np.random.uniform() < 0.05:\n",
      "        action = random.randint(0, action_num - 1)\n",
      "    isTerminal = ale.game_over()\n",
      "    #print(np.max(current_DQN))\n",
      "    #print(action)\n",
      "\n",
      "    reward = ale.act(action_map[action])\n",
      "    #print(reward)\n",
      "    R += reward\n",
      "    c += 1\n",
      "plt.plot(maxQ, color=\"red\")\n",
      "plt.plot(Qs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in range(1000):\n",
      "    update_params2()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 412
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def update_params2():\n",
      "    if global_step > STEPS_BEFORE_TRAINING:\n",
      "        state_batch, action_batch, reward_batch, next_state_batch, terminal_batch, _ = RM2.sample_transition_batch()\n",
      "        if global_step % SAVE_SUMMARY_STEPS == 0:\n",
      "            DQNT_max_action_batch, DQNT_summary_str = sess.run([max_DQNT, DQNT_summary_op], feed_dict={DQNT_input_placeholder:next_state_batch})\n",
      "        else:\n",
      "            DQNT_max_action_batch = sess.run(max_DQNT, feed_dict={DQNT_input_placeholder:next_state_batch})\n",
      "\n",
      "        Y = []\n",
      "        for i in range(state_batch.shape[0]):\n",
      "            terminal = terminal_batch[i]\n",
      "            if terminal:\n",
      "                Y.append(reward_batch[i])\n",
      "            else:\n",
      "                Y.append(reward_batch[i] + GAMMA * DQNT_max_action_batch[i])\n",
      "                \n",
      "        if global_step % SAVE_SUMMARY_STEPS == 0:\n",
      "            _, DQN_summary_str = sess.run([train_op, DQN_summary_op], feed_dict={Y_placeholder : Y,\n",
      "                                                              Action_batch : action_batch,\n",
      "                                                              DQN_input_placeholder : state_batch})\n",
      "        else:\n",
      "             _ = sess.run(train_op, feed_dict={Y_placeholder : Y,\n",
      "                                                              Action_batch : action_batch,\n",
      "                                                              DQN_input_placeholder : state_batch})           \n",
      "            \n",
      "        if global_step % SAVE_SUMMARY_STEPS == 0:\n",
      "            summary_writter.add_summary(DQNT_summary_str, global_step)\n",
      "            summary_writter.add_summary(DQN_summary_str, global_step)\n",
      "\n",
      "        if global_step % PARAM_SYNC_STEPS == 0:\n",
      "            sess.run(sync_DQNT_op)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 245
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "1"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ale.act(actio)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ale-old.ipynb."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}