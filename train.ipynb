{
 "metadata": {
  "name": "",
  "signature": "sha256:eb5120dcbe4a595b60bcfb5c02b442cfeda538182e9ed228c5b1eced9bb19621"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from ale_python_interface import ALEInterface\n",
      "import tensorflow as tf\n",
      "import numpy as np\n",
      "import cv2\n",
      "import random\n",
      "from collections import deque\n",
      "\n",
      "import time\n",
      "import os\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "\n",
      "from replayMemory import ReplayMemory\n",
      "import buildGraph"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/home/cgel/.local/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
        "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ale = ALEInterface()\n",
      "viz = False\n",
      "rom_name = \"roms/Breakout.bin\"\n",
      "ale.setBool('sound', False)                                                                                \n",
      "ale.setBool('display_screen', viz) \n",
      "ale.setInt(\"frame_skip\", 4)\n",
      "ale.loadROM(rom_name)\n",
      "legal_actions = ale.getMinimalActionSet()\n",
      "action_map = {}\n",
      "for i in range(len(legal_actions)):\n",
      "    action_map[i] = legal_actions[i]\n",
      "action_num = len(action_map)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "STEPS_BEFORE_TRAINING = 100\n",
      "EXPLORE = 1000000. # frames over which to anneal epsilon\n",
      "FINAL_EPSILON = 0.1#0.001 # final value of epsilon\n",
      "INITIAL_EPSILON = 1.0#0.01 # starting value of epsilon\n",
      "BATCH_SIZE = 32 # size of minibatch\n",
      "PARAM_SYNC_STEPS = 10000\n",
      "BUF_SIZE = 4\n",
      "SAVE_SUMMARY_STEPS = 1000\n",
      "\n",
      "class config:\n",
      "    batch_size = 32\n",
      "    replay_memory_capacity = 1000000\n",
      "    buff_size = BUF_SIZE\n",
      "    \n",
      "def get_epsilon():\n",
      "    if global_step < EXPLORE:\n",
      "        return INITIAL_EPSILON - ( (INITIAL_EPSILON-FINAL_EPSILON)/EXPLORE ) * global_step\n",
      "    else:\n",
      "        return FINAL_EPSILON\n",
      "    \n",
      "RM = ReplayMemory(config)\n",
      "\n",
      "learning_rate = 0.00025\n",
      "GAMMA = 0.96"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def preprocess(new_frame, state):\n",
      "    frame = cv2.resize(new_frame, (84, 110))[26:110,:]\n",
      "    new_state = np.roll(state, -1, axis=3)\n",
      "    new_state[0, :, :, BUF_SIZE -1] = frame\n",
      "    return new_state"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with tf.device(\"/gpu:0\"):\n",
      "    Action_batch = tf.placeholder(tf.int64, [None], name=\"Action_placholder\")\n",
      "    with tf.variable_scope(\"DQN\"):\n",
      "        DQN_input_placeholder, DQN, future_DQN, DQN_params = buildGraph.createQNetwork(\"DQN_summaries\", action_num, Action_batch)\n",
      "        #the action with the highest Q$\n",
      "        max_action_DQN = tf.argmax(DQN, 1)\n",
      "        #tf.scalar_summary(\"max_DQN\", max_action_DQN)\n",
      "    with tf.variable_scope(\"DQNTarget\"):\n",
      "        DQNT_input_placeholder, DQNT, future_DQNT, DQNT_params = buildGraph.createQNetwork(\"DQNT_summaries\", action_num, Action_batch)\n",
      "        #the higest DQNT value\n",
      "        max_DQNT = tf.reduce_max(DQNT, 1)\n",
      "\n",
      "    # DQN summary\n",
      "    for i in range(action_num):\n",
      "        dqni = tf.scalar_summary(\"DQN/action\"+str(i), DQN[0, i])\n",
      "        tf.add_to_collection(\"DQN_summaries\", dqni)\n",
      "\n",
      "    sync_DQNT_op = [DQNT_params[i].assign(DQN_params[i]) for i in range(len(DQN_params))]\n",
      "\n",
      "    # r + Qtarget; is feeded as Y\n",
      "    Y_placeholder = tf.placeholder(tf.float32, [None], name=\"Y_placeholder\")\n",
      "    next_max_Q_placeholder = tf.placeholder(tf.float32, [None], name=\"next_max_Q_placeholder\")\n",
      "    train_op = buildGraph.build_train_op(DQN, Y_placeholder, Action_batch, future_DQN, next_max_Q_placeholder, action_num, learning_rate)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def update_params():\n",
      "    if global_step > STEPS_BEFORE_TRAINING:\n",
      "        state_batch, action_batch, reward_batch, next_state_batch, terminal_batch, _ = RM.sample_transition_batch()\n",
      "        if global_step % SAVE_SUMMARY_STEPS == 0:\n",
      "            DQNT_max_action_batch, DQNT_summary_str = sess.run([max_DQNT, DQNT_summary_op], feed_dict={DQNT_input_placeholder:next_state_batch})\n",
      "        else:\n",
      "            DQNT_max_action_batch = sess.run(max_DQNT, feed_dict={DQNT_input_placeholder:next_state_batch})\n",
      "\n",
      "        Y = []\n",
      "        for i in range(state_batch.shape[0]):\n",
      "            terminal = terminal_batch[i]\n",
      "            if terminal:\n",
      "                Y.append(reward_batch[i])\n",
      "            else:\n",
      "                Y.append(reward_batch[i] + GAMMA * DQNT_max_action_batch[i])\n",
      "        \n",
      "        DQN_feed_dict={Y_placeholder : Y, Action_batch : action_batch, DQN_input_placeholder : state_batch, next_max_Q_placeholder:DQNT_max_action_batch}\n",
      "        if global_step % SAVE_SUMMARY_STEPS == 0:\n",
      "            _, DQN_summary_str = sess.run([train_op, DQN_summary_op], feed_dict=DQN_feed_dict)\n",
      "        else:\n",
      "             _ = sess.run(train_op, feed_dict=DQN_feed_dict)           \n",
      "            \n",
      "        if global_step % SAVE_SUMMARY_STEPS == 0:\n",
      "            summary_writter.add_summary(DQNT_summary_str, global_step)\n",
      "            summary_writter.add_summary(DQN_summary_str, global_step)\n",
      "\n",
      "        if global_step % PARAM_SYNC_STEPS == 0:\n",
      "            sess.run(sync_DQNT_op)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sess = tf.InteractiveSession(config=tf.ConfigProto(\n",
      "        allow_soft_placement=True))\n",
      "saver = tf.train.Saver(max_to_keep = 20)\n",
      "sess.run(tf.initialize_all_variables())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#geneate a new set of paths\n",
      "run_list = os.listdir(\"log\")\n",
      "int_run_list = [int(r) for r in run_list] + [0]\n",
      "run_name = str(max(int_run_list) + 1)\n",
      "#run_name = str(3)\n",
      "checkpoint_path = \"checkpoint/\" + run_name + \".ckpt\"\n",
      "log_path = \"log/\"+ run_name\n",
      "print(run_name)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "86\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "DQN_summary_op = tf.merge_summary(tf.get_collection(\"DQN_summaries\") + tf.get_collection(\"DQN_summaries_prediction\"))\n",
      "DQNT_summary_op = tf.merge_summary(tf.get_collection(\"DQNT_summaries\"))\n",
      "summary_writter = tf.train.SummaryWriter(log_path, sess.graph, flush_secs=20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "saver.restore(sess, \"checkpoint/71.ckpt-8000\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def e_greedy_action(epsilon, state):\n",
      "        if np.random.uniform() < epsilon:\n",
      "            action = random.randint(0, action_num - 1)\n",
      "        else:\n",
      "            action = np.argmax(sess.run(DQN, feed_dict={DQN_input_placeholder:state})[0])\n",
      "        return action\n",
      "    \n",
      "def greedy_run(epsilon, n):\n",
      "    ale.reset_game()\n",
      "    R_list = []\n",
      "    for episode in range(n):\n",
      "        state = np.zeros((1, 84, 84, BUF_SIZE), dtype=np.uint8)\n",
      "        state = preprocess(ale.getScreenGrayscale(), state)\n",
      "        R = 0\n",
      "        while ale.game_over() == False:\n",
      "            action = e_greedy_action(epsilon, state)\n",
      "            reward = ale.act(action_map[action])\n",
      "            state = preprocess(ale.getScreenGrayscale(), state)\n",
      "            R += reward\n",
      "        R_list.append(R)\n",
      "        ale.reset_game()\n",
      "    return R_list"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "global_step = 0\n",
      "global_episode = 0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t = time.time()\n",
      "num_episodes = 100000\n",
      "initial_episode = global_episode\n",
      "for episode in range(global_episode, num_episodes + global_episode):\n",
      "    global state\n",
      "    state = np.zeros((1, 84, 84, BUF_SIZE), dtype=np.uint8)\n",
      "    state = preprocess(ale.getScreenGrayscale(), state)\n",
      "    R = 0\n",
      "    ep_begin_t = time.time()\n",
      "    isTerminal = False\n",
      "    episode_begining_step = global_step\n",
      "    while isTerminal == False:\n",
      "        action = e_greedy_action(get_epsilon(), state)\n",
      "        reward = ale.act(action_map[action])\n",
      "        clipped_reward = max(-1, min(1, reward))\n",
      "        R += reward\n",
      "        if ale.game_over():\n",
      "            isTerminal = True\n",
      "        RM.add(state[0, :, :, BUF_SIZE -1], action, clipped_reward, isTerminal)\n",
      "        update_params()\n",
      "        state = preprocess(ale.getScreenGrayscale(), state)\n",
      "        global_step += 1\n",
      "    ep_duration = time.time() - ep_begin_t\n",
      "    if episode%10 == 0:\n",
      "        episode_online_summary = tf.Summary(value=[tf.Summary.Value(tag=\"online/epsilon\", simple_value=get_epsilon()), \n",
      "                                    tf.Summary.Value(tag=\"online/R\", simple_value=R),\n",
      "                                    tf.Summary.Value(tag=\"online/steps_in_episode\", simple_value= global_step - episode_begining_step),       \n",
      "                                    tf.Summary.Value(tag=\"online/ep_duration_seconds\", simple_value=ep_duration)])\n",
      "        summary_writter.add_summary(episode_online_summary, global_episode)\n",
      "    # log percent\n",
      "    if episode%500 == 0:\n",
      "        percent = int(float(episode - initial_episode)/num_episodes * 100)\n",
      "        print(\"%i%% -- epsilon:%.2f\"%(percent, get_epsilon()))\n",
      "        \n",
      "    # save\n",
      "    if episode%1000 == 0 and episode != 0 or num_episodes == episode:\n",
      "        print(\"saving checkpoint at episode \" + str(episode))\n",
      "        saver.save(sess, checkpoint_path, episode)\n",
      "        \n",
      "    # performance summary\n",
      "    if episode%100 == 0:\n",
      "        R_list = greedy_run(epsilon = 0.05, n=10)\n",
      "        performance_summary = tf.Summary(value=[tf.Summary.Value(tag=\"R/average\", simple_value=sum(R_list)/len(R_list)),\n",
      "                                      tf.Summary.Value(tag=\"R/max\", simple_value=max(R_list)),\n",
      "                                      tf.Summary.Value(tag=\"R/min\", simple_value=min(R_list))])\n",
      "        summary_writter.add_summary(performance_summary, global_step)\n",
      "        \n",
      "    global_episode += 1\n",
      "    ale.reset_game()\n",
      "    \n",
      "print(\"==\")\n",
      "print((time.time() - t)/60)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "InvalidArgumentError",
       "evalue": "AttrValue must not have reference type value of float_ref\n\t for attr 'tensor_type'\n\t; NodeDef: DQN/conditional_linear1_W/RMSProp_1/_56 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_563_DQN/conditional_linear1_W/RMSProp_1\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](^RMSProp/learning_rate/_40, ^RMSProp/decay/_42, ^RMSProp/momentum/_44, ^RMSProp/epsilon/_46, ^gradients/DQN/Gather_grad/Reshape/_58, ^gradients/DQN/Gather_grad/Reshape_1/_60); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>\n\t [[Node: DQN/conditional_linear1_W/RMSProp_1/_56 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_563_DQN/conditional_linear1_W/RMSProp_1\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](^RMSProp/learning_rate/_40, ^RMSProp/decay/_42, ^RMSProp/momentum/_44, ^RMSProp/epsilon/_46, ^gradients/DQN/Gather_grad/Reshape/_58, ^gradients/DQN/Gather_grad/Reshape_1/_60)]]",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-10-d295a1a2aed5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0misTerminal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mRM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBUF_SIZE\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclipped_reward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0misTerminal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mupdate_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0male\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetScreenGrayscale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mglobal_step\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-6-a7f6a933fb53>\u001b[0m in \u001b[0;36mupdate_params\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDQN_summary_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDQN_summary_op\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDQN_feed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m              \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDQN_feed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mSAVE_SUMMARY_STEPS\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    708\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    709\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 710\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    711\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    712\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    906\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 908\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    909\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    910\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    956\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    957\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m--> 958\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m    959\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    960\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m    976\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    977\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 978\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    979\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    980\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mInvalidArgumentError\u001b[0m: AttrValue must not have reference type value of float_ref\n\t for attr 'tensor_type'\n\t; NodeDef: DQN/conditional_linear1_W/RMSProp_1/_56 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_563_DQN/conditional_linear1_W/RMSProp_1\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](^RMSProp/learning_rate/_40, ^RMSProp/decay/_42, ^RMSProp/momentum/_44, ^RMSProp/epsilon/_46, ^gradients/DQN/Gather_grad/Reshape/_58, ^gradients/DQN/Gather_grad/Reshape_1/_60); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>\n\t [[Node: DQN/conditional_linear1_W/RMSProp_1/_56 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_563_DQN/conditional_linear1_W/RMSProp_1\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](^RMSProp/learning_rate/_40, ^RMSProp/decay/_42, ^RMSProp/momentum/_44, ^RMSProp/epsilon/_46, ^gradients/DQN/Gather_grad/Reshape/_58, ^gradients/DQN/Gather_grad/Reshape_1/_60)]]"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "RM_path = \"checkpoint/RM_\"+run_name+\"_\"+str(global_step)\n",
      "def RM_save(self):\n",
      "    name_array = zip(['actions', 'rewards',  'screens',  'terminals',  'state_batch',  'next_state_batch'],\n",
      "        [self.actions, self.rewards, self.screens, self.terminals, self.state_batch, self.next_state_batch])\n",
      "    for idx, (name, array) in enumerate(name_array):\n",
      "        np.save(RM_path+name, array)\n",
      "    \n",
      "def RM_load():\n",
      "    self = ReplayMemory(config)\n",
      "    name_array = zip(['actions', 'rewards',  'screens',  'terminals',  'state_batch',  'next_state_batch'],\n",
      "        [self.actions, self.rewards, self.screens, self.terminals, self.state_batch, self.next_state_batch])\n",
      "    self.filled = True\n",
      "    for idx, (name, array) in enumerate(name_array):\n",
      "        array[:] = np.load(RM_path+name+\".npy\")\n",
      "    return self\n",
      "\n",
      "def RM_info(RM):\n",
      "    print(\"current :\"+str(RM.current))\n",
      "    print(\"step :\"+str(RM.step))\n",
      "    print(\"capacity :\"+str(RM.capacity))\n",
      "    print(\"filled :\"+str(RM.filled))\n",
      "    print(\"batch_size :\"+str(RM.batch_size))\n",
      "    \n",
      "print(RM_path)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "checkpoint/RM_29_1640904\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "saver.save(sess, checkpoint_path, episode)\n",
      "print(checkpoint_path)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "checkpoint/62.ckpt\n"
       ]
      }
     ],
     "prompt_number": 238
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "saver.restore(sess, checkpoint_path+\"-\"+str(episode))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 468
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "greedy_run(0.05, 2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 470,
       "text": [
        "[9, 5]"
       ]
      }
     ],
     "prompt_number": 470
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in range(10000):\n",
      "    update_params()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 471
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "color = \"Greys_r\"\n",
      "def show_state(state):\n",
      "    fig = plt.figure()\n",
      "    for i in range(0, 4):\n",
      "        a=fig.add_subplot(1,4,i+1)\n",
      "        plt.axis(\"off\")\n",
      "        plt.title(str(i))\n",
      "        plt.imshow(state[:,:,i], color)\n",
      "\n",
      "def show_frame(frame):\n",
      "    fig = plt.figure()\n",
      "    plt.axis(\"off\")\n",
      "    if len(frame.shape) == 2:\n",
      "        plt.imshow(frame, color)\n",
      "    elif len(frame.shape) == 3:\n",
      "        plt.imshow(frame[:,:,BUF_SIZE -1], color)\n",
      "    elif len(frame.shape) == 4:\n",
      "        plt.imshow(frame[0, :,:,BUF_SIZE -1], color)\n",
      "    else:\n",
      "        print(\"Wrong shape\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 239
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "RM2 = ReplayMemory(config)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 214
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "maxQ = []\n",
      "Qs = []"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ale.reset_game()\n",
      "state = np.zeros((1, 84, 84, BUF_SIZE), dtype=np.uint8)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "c = 0\n",
      "while ale.game_over() == False and c < 10:\n",
      "    raw_frame = ale.getScreenGrayscale()\n",
      "    prev_state = state\n",
      "    state = preprocess(raw_frame, state)\n",
      "    current_DQN =  sess.run(DQN, feed_dict={DQN_input_placeholder:state})[0]\n",
      "    Qs.append(current_DQN)\n",
      "    maxQ.append(np.max(current_DQN))\n",
      "    action = np.argmax(current_DQN)\n",
      "    if np.random.uniform() < 0.05:\n",
      "        action = random.randint(0, action_num - 1)\n",
      "    isTerminal = ale.game_over()\n",
      "    #print(np.max(current_DQN))\n",
      "    #print(action)\n",
      "\n",
      "    reward = ale.act(action_map[action])\n",
      "    #print(reward)\n",
      "    R += reward\n",
      "    c += 1\n",
      "plt.plot(maxQ, color=\"red\")\n",
      "plt.plot(Qs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in range(1000):\n",
      "    update_params2()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 412
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def update_params2():\n",
      "    if global_step > STEPS_BEFORE_TRAINING:\n",
      "        state_batch, action_batch, reward_batch, next_state_batch, terminal_batch, _ = RM2.sample_transition_batch()\n",
      "        if global_step % SAVE_SUMMARY_STEPS == 0:\n",
      "            DQNT_max_action_batch, DQNT_summary_str = sess.run([max_DQNT, DQNT_summary_op], feed_dict={DQNT_input_placeholder:next_state_batch})\n",
      "        else:\n",
      "            DQNT_max_action_batch = sess.run(max_DQNT, feed_dict={DQNT_input_placeholder:next_state_batch})\n",
      "\n",
      "        Y = []\n",
      "        for i in range(state_batch.shape[0]):\n",
      "            terminal = terminal_batch[i]\n",
      "            if terminal:\n",
      "                Y.append(reward_batch[i])\n",
      "            else:\n",
      "                Y.append(reward_batch[i] + GAMMA * DQNT_max_action_batch[i])\n",
      "                \n",
      "        if global_step % SAVE_SUMMARY_STEPS == 0:\n",
      "            _, DQN_summary_str = sess.run([train_op, DQN_summary_op], feed_dict={Y_placeholder : Y,\n",
      "                                                              Action_batch : action_batch,\n",
      "                                                              DQN_input_placeholder : state_batch})\n",
      "        else:\n",
      "             _ = sess.run(train_op, feed_dict={Y_placeholder : Y,\n",
      "                                                              Action_batch : action_batch,\n",
      "                                                              DQN_input_placeholder : state_batch})           \n",
      "            \n",
      "        if global_step % SAVE_SUMMARY_STEPS == 0:\n",
      "            summary_writter.add_summary(DQNT_summary_str, global_step)\n",
      "            summary_writter.add_summary(DQN_summary_str, global_step)\n",
      "\n",
      "        if global_step % PARAM_SYNC_STEPS == 0:\n",
      "            sess.run(sync_DQNT_op)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 245
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "1"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ale.act(actio)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ale-old.ipynb."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}